[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "IMPACT OF MACROECONOMIC FACTORS ON EMPLOYMENT MARKET",
    "section": "",
    "text": "What is a Time Series ?\n\nAny metric that is measured over regular time intervals makes a Time Series.\n\nExample: Weather data, Stock prices, Industry forecasts, etc are some of the common ones.\n\nThe analysis of experimental data that have been observed at different points in time leads to new and unique problems in statistical modeling and inference.\nThe obvious correlation introduced by the sampling of adjacent points in time can severely restrict the applicability of the many conventional statistical methods traditionally dependent on the assumption that these adjacent observations are independent and identically distributed."
  },
  {
    "objectID": "viz.html#employment",
    "href": "viz.html#employment",
    "title": "Data Visualization",
    "section": "Employment:",
    "text": "Employment:\n\n\n\n\n\n\n\n\n\nBetween 2003 and 2023, the US labor market experienced both periods of growth and contraction, with employment trends varying widely across different industries. Here is a brief overview of the employment trends in some of the major industries in the US during this time period:\nHealthcare and Social Assistance: The healthcare and social assistance industry has consistently been one of the fastest-growing sectors of the US economy over the past two decades. Employment in this industry has grown steadily since 2003, and is projected to continue to grow at a faster rate than most other industries through 2023.\nProfessional and Business Services: This industry includes a wide range of occupations such as accounting, legal services, and management consulting. Employment in this sector has also grown steadily since 2003, and is expected to continue to do so through 2023.\nManufacturing: The manufacturing industry has experienced significant fluctuations in employment over the past two decades. Between 2003 and 2009, employment in manufacturing declined sharply, due in part to increased automation and offshoring of jobs. However, since 2010, employment in this sector has rebounded somewhat, with modest growth projected through 2023.\nRetail Trade: Employment in the retail trade sector has been more volatile than most other industries over the past two decades. Between 2003 and 2009, employment in this sector grew steadily, but since then has declined somewhat due to increased competition from online retailers and changing consumer preferences. Despite this, retail trade remains a major employer in the US, and is projected to continue to employ a large number of workers through 2023.\nFinancial Activities: This industry includes banking, insurance, and other financial services. Employment in this sector has fluctuated somewhat over the past two decades, with declines during the financial crisis of 2008-2009, followed by modest growth since then. Employment in financial activities is expected to continue to grow at a moderate pace through 2023."
  },
  {
    "objectID": "viz.html#gdp",
    "href": "viz.html#gdp",
    "title": "Data Visualization",
    "section": "GDP",
    "text": "GDP\n\n\n\n\n\n\n\n\n\nBetween 2003 and 2022, the US economy experienced both periods of growth and contraction. The period from 2003 to 2007 saw a steady increase in GDP, with an average annual growth rate of 2.6%. However, the global financial crisis of 2008-2009 caused a sharp decline in economic activity, with GDP falling by 2.8% in 2009.\nThe US economy recovered slowly in the years that followed, with GDP growth averaging 1.8% per year from 2010 to 2019. However, the COVID-19 pandemic in 2020 caused a severe contraction in economic activity, with GDP falling by a record 3.5% in 2020. The US economy has since begun to recover, with GDP growth of 5.7% in the third quarter of 2021.\nThroughout this period, the US economy has been driven by a variety of factors, including changes in consumer spending, business investment, government spending, and international trade. Additionally, the growth of industries such as technology, healthcare, and energy has played a significant role in shaping the US economy over the past two decades."
  },
  {
    "objectID": "viz.html#interest-rate",
    "href": "viz.html#interest-rate",
    "title": "Data Visualization",
    "section": "Interest Rate",
    "text": "Interest Rate\n\n\n\n\n\n\nBetween 1987 and 2020, the US economy experienced both high and low-interest rates, with periods of stability and volatility. During the late 1980s and early 1990s, interest rates were generally high, with the federal funds rate reaching a peak of 9.75% in May 1989.\nIn the mid to late 1990s, interest rates began to decline, with the federal funds rate reaching a low of 1% in June 2003. This period of low-interest rates continued until the mid-2000s, when rates began to rise again, reaching a peak of 5.25% in June 2006.\nThe global financial crisis of 2008-2009 caused a sharp decline in interest rates, with the federal funds rate falling to near-zero levels in December 2008. Interest rates remained low throughout the 2010s, with the federal funds rate hovering between 0% and 0.25% for much of the decade.\nIn 2015, the Federal Reserve began a process of gradual interest rate hikes, raising the federal funds rate to a peak of 2.5% in December 2018. However, in response to slowing economic growth and rising trade tensions, the Federal Reserve began to lower interest rates again in 2019, with the federal funds rate falling to a range of 1.50% to 1.75% by the end of the year.\nIn 2020, the COVID-19 pandemic caused a sharp decline in economic activity and led to a further reduction in interest rates. The Federal Reserve cut interest rates to near-zero levels and launched a series of monetary policy measures to support the economy, including quantitative easing and lending programs for businesses and local governments."
  },
  {
    "objectID": "eda.html#original-time-series",
    "href": "eda.html#original-time-series",
    "title": "Exploratory Data Analysis",
    "section": "Original Time Series",
    "text": "Original Time Series\n\n\n\n\n\n\n\n\nIn the early 2000s, the information sector was growing very fast, with employment increasing rapidly due to the growth of the internet and related technologies. Since then, the information sector has experienced a mixed employment performance. Between 2003 and 2007, employment in the sector increased moderately. However, the Great Recession of 2008-2009 led to significant job losses in the sector, with employment falling sharply from 2008 to 2010.\nSince then, employment in the information sector has been gradually recovering. From 2010 to 2019, employment in the sector increased by an average of 2.3% annually, outpacing the overall employment growth in the US economy. During this time, the software and computer services sub-sector has been the fastest-growing within the information sector, driven by increased demand for cloud computing and other software-related services.\nHowever, the COVID-19 pandemic has had a mixed impact on employment in the information sector. While some sub-sectors, such as software development and online content creation, have seen increased demand and employment growth during the pandemic, others, such as publishing and broadcasting, have seen significant job losses due to disruptions in advertising revenue and other factors."
  },
  {
    "objectID": "eda.html#lag-plot",
    "href": "eda.html#lag-plot",
    "title": "Exploratory Data Analysis",
    "section": "Lag Plot",
    "text": "Lag Plot\n\n\n\n\n\nFrom those lag plots, we can observe only lag 1, 12, 24, 36, 48, 60 are correlated which means there is seasonal correlation. However, other lag plots aren’t correlated."
  },
  {
    "objectID": "eda.html#decomposition",
    "href": "eda.html#decomposition",
    "title": "Exploratory Data Analysis",
    "section": "Decomposition",
    "text": "Decomposition\n\n\n\n\n\nSimilar to the result above, we could observed an upward trend in the U.S. employment data set. However, there’s fluctuation in 2008 & 2019."
  },
  {
    "objectID": "eda.html#acf-and-pacf",
    "href": "eda.html#acf-and-pacf",
    "title": "Exploratory Data Analysis",
    "section": "ACF and PACF",
    "text": "ACF and PACF\n\n\n\n\n\nBy looking at the ACF, it can be concluded that the series is not Stationary. In this graph lag1, lag12, lag 24 is higher than for the other lags. This is due to the seasonal pattern in the data, the peaks tend to be 12 months apart and the troughs tend to be 12 months apart as well. Since the dashed blue lines indicate whether the correlations are significantly different from zero. Most of the lage exceed the dashed blue line, so that the data isn’t stationary."
  },
  {
    "objectID": "eda.html#augmented-dickey-fuller-test",
    "href": "eda.html#augmented-dickey-fuller-test",
    "title": "Exploratory Data Analysis",
    "section": "Augmented Dickey-Fuller Test",
    "text": "Augmented Dickey-Fuller Test\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  emp_ts\nDickey-Fuller = -1.8826, Lag order = 6, p-value = 0.6252\nalternative hypothesis: stationary\n\n\nP-value is larger than 0.05. We don’t have enough evidence to reject the null hypothesis at 5% significance level. So the ADF test thought the series is not stationary. This result supports the result from ACF and PACF plot."
  },
  {
    "objectID": "eda.html#make-data-stationary",
    "href": "eda.html#make-data-stationary",
    "title": "Exploratory Data Analysis",
    "section": "Make Data Stationary",
    "text": "Make Data Stationary\n\n\n\n\n\n\n\n\n\n\nFrom the line plot and ACF plot, we can observe that the result of taking first & second order differential transformation is much better than the result of detrending. The original chart shows the data set is much more stationary since the mean and variance is approximately constant. Also, from the ACF chart we could observe most of the correlation are within the range."
  },
  {
    "objectID": "eda.html#moving-average-smoothing",
    "href": "eda.html#moving-average-smoothing",
    "title": "Exploratory Data Analysis",
    "section": "Moving Average Smoothing",
    "text": "Moving Average Smoothing\n\nautoplot(emp_ts) +\n  autolayer(ma(emp_ts,12), series=\"12-MA\") +\n  autolayer(ma(emp_ts,36), series=\"36-MA\") +\n  autolayer(ma(emp_ts,120), series=\"120-MA\") +\n  xlab(\"Date\") + ylab(\"Employment\") +\n  ggtitle(\"Moving Average Smoothing of U.S. Employment\")\n\n\n\n\nUsing 12, 36, 120 MA window indicates there’s an upward trend in the NBA Web Search data set."
  },
  {
    "objectID": "deepLearning.html",
    "href": "deepLearning.html",
    "title": "Time Series",
    "section": "",
    "text": "from pandas import read_csv\nimport numpy as np\nfrom keras.models import Sequential\nfrom keras.layers import Dense, SimpleRNN, Bidirectional, GRU, LSTM\nfrom keras import layers\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error\nimport math\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras import regularizers\n\n\n# Parameter split_percent defines the ratio of training examples\ndef get_train_test(df, split_percent=0.8):\n    #df = read_csv(url, usecols=['Adj Close'], engine='python')\n    data = np.array(df.values.astype('float32'))\n    scaler = MinMaxScaler(feature_range=(0, 1))\n    data = scaler.fit_transform(data).flatten()\n    n = len(data)\n    # Point for splitting data into train and test\n\n    split = int(n * split_percent)\n    train_data = data[range(split)]\n    test_data = data[split:]\n    return train_data, test_data, data\n\n\n# Prepare the input X and target Y\ndef get_XY(dat, time_steps):\n    # Indices of target array\n    Y_ind = np.arange(time_steps, len(dat), time_steps)\n    Y = dat[Y_ind]\n    # Prepare X\n    rows_x = len(Y)\n    X = dat[range(time_steps * rows_x)]\n    X = np.reshape(X, (rows_x, time_steps, 1))\n    return X, Y\n\n\n# RNN model\ndef create_RNN(hidden_units, dense_units, input_shape, activation):\n    model = Sequential()\n    model.add(SimpleRNN(hidden_units, input_shape=input_shape,\n                        activation=activation[0]))\n    model.add(Dense(units=dense_units, activation=activation[1],kernel_regularizer=regularizers.L1L2(l1=1e-5, l2=1e-4),\n                      bias_regularizer=regularizers.L2(1e-4),\n                      activity_regularizer=regularizers.L2(1e-5)))\n    model.compile(loss='mean_squared_error', optimizer='adam',run_eagerly=True)\n    return model\n\n\n# LSTM model\ndef create_LSTM():\n    model = Sequential()\n    model.add(Bidirectional(LSTM(32, activation='tanh')))\n    model.add(layers.Dense(1, activation='linear',kernel_regularizer=regularizers.L1L2(l1=1e-5, l2=1e-4),\n                      bias_regularizer=regularizers.L2(1e-4),\n                      activity_regularizer=regularizers.L2(1e-5)))\n    model.compile(optimizer=\"adam\", loss='mae',run_eagerly=True)\n    return model\n\n\n# GRU model\ndef create_GRU():\n    model = Sequential()\n    model.add(Bidirectional(GRU(units=32, activation='tanh', recurrent_activation='sigmoid', stateful=False)))\n    model.add(layers.Dense(1, activation='linear',kernel_regularizer=regularizers.L1L2(l1=1e-5, l2=1e-4),\n                      bias_regularizer=regularizers.L2(1e-4),\n                      activity_regularizer=regularizers.L2(1e-5)))\n    model.compile(optimizer=\"adam\", loss='mae',run_eagerly=True)\n    return model\n\n\n# Evaluation matrices\ndef print_error(trainY, testY, train_predict, test_predict):\n    # Error of predictions\n    train_rmse = math.sqrt(mean_squared_error(trainY, train_predict))\n    test_rmse = math.sqrt(mean_squared_error(testY, test_predict))\n    \n    train_mse = mean_squared_error(trainY, train_predict)\n    test_mse = mean_squared_error(testY, test_predict)\n    \n    # Print MSE\n    print(\"Train MSE: %.3f MSE\" % (train_mse))\n    print(\"Train MSE: %.3f MSE\" % (test_mse))\n    \n    # Print RMSE\n    print('Train RMSE: %.3f RMSE' % (train_rmse))\n    print('Test RMSE: %.3f RMSE' % (test_rmse))\n    \n    \n\n# Plot the result\ndef plot_result(trainY, testY, train_predict, test_predict):\n    actual = np.append(trainY, testY)\n    predictions = np.append(train_predict, test_predict)\n    rows = len(actual)\n    plt.figure(figsize=(15, 6), dpi=80)\n    plt.plot(range(rows), actual)\n    plt.plot(range(rows), predictions)\n    plt.legend(['Actual', 'Predictions'])\n    plt.xlabel('Observation number after given time steps')\n    plt.ylabel('Interest Rate')\n    plt.title('Model Prediction Result')\n\n\nfrom google.colab import files\nuploaded = files.upload()\n\n\n     \n     \n      Upload widget is only available when the cell has been executed in the\n      current browser session. Please rerun this cell to enable.\n      \n       \n\n\nSaving data.csv to data.csv\n\n\n\nimport io\ndf = read_csv(io.BytesIO(uploaded['data.csv']))\n# Dataset is now stored in a Pandas Dataframe\n\n\ndf = df.drop(['Unnamed: 0', 'DATE'], axis=1)\ndf\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      GDP\n      Information Employment\n      Interest Rate\n    \n  \n  \n    \n      0\n      11312.766\n      3208\n      1.309618\n    \n    \n      1\n      11566.669\n      3176\n      1.128689\n    \n    \n      2\n      11772.234\n      3155\n      1.380819\n    \n    \n      3\n      11923.447\n      3141\n      1.701954\n    \n    \n      4\n      12112.815\n      3130\n      1.358149\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      74\n      24349.121\n      2934\n      0.087901\n    \n    \n      75\n      24740.480\n      2985\n      0.319495\n    \n    \n      76\n      25248.476\n      3035\n      0.949400\n    \n    \n      77\n      25723.941\n      3102\n      1.062175\n    \n    \n      78\n      26137.992\n      3116\n      1.797332\n    \n  \n\n79 rows × 3 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n\n\ntrain_data, test_data, data = get_train_test(df)\ntime_steps = 12\ntrainX, trainY = get_XY(train_data, time_steps)\ntestX, testY = get_XY(test_data, time_steps)\n\n\n\n\n\nmodel = create_RNN(hidden_units=3, dense_units=1, input_shape=(time_steps, 1),\n                       activation=['tanh', 'tanh'])\n\nmodel.fit(trainX, trainY, epochs=20, batch_size=1, verbose=2)\nmodel.summary()\n\n# make predictions\ntrain_predict = model.predict(trainX)\ntest_predict = model.predict(testX)\n# Mean square error\nprint_error(trainY, testY, train_predict, test_predict)\n# plot the result [Actual and Predicted Values]\nplot_result(trainY, testY, train_predict, test_predict)\n\nEpoch 1/20\n15/15 - 1s - loss: 0.1122 - 843ms/epoch - 56ms/step\nEpoch 2/20\n15/15 - 1s - loss: 0.1037 - 780ms/epoch - 52ms/step\nEpoch 3/20\n15/15 - 1s - loss: 0.0963 - 763ms/epoch - 51ms/step\nEpoch 4/20\n15/15 - 1s - loss: 0.0904 - 780ms/epoch - 52ms/step\nEpoch 5/20\n15/15 - 1s - loss: 0.0853 - 814ms/epoch - 54ms/step\nEpoch 6/20\n15/15 - 1s - loss: 0.0796 - 800ms/epoch - 53ms/step\nEpoch 7/20\n15/15 - 1s - loss: 0.0744 - 779ms/epoch - 52ms/step\nEpoch 8/20\n15/15 - 1s - loss: 0.0698 - 1s/epoch - 71ms/step\nEpoch 9/20\n15/15 - 1s - loss: 0.0647 - 1s/epoch - 69ms/step\nEpoch 10/20\n15/15 - 1s - loss: 0.0597 - 927ms/epoch - 62ms/step\nEpoch 11/20\n15/15 - 1s - loss: 0.0548 - 793ms/epoch - 53ms/step\nEpoch 12/20\n15/15 - 1s - loss: 0.0498 - 789ms/epoch - 53ms/step\nEpoch 13/20\n15/15 - 1s - loss: 0.0454 - 783ms/epoch - 52ms/step\nEpoch 14/20\n15/15 - 1s - loss: 0.0412 - 771ms/epoch - 51ms/step\nEpoch 15/20\n15/15 - 1s - loss: 0.0376 - 771ms/epoch - 51ms/step\nEpoch 16/20\n15/15 - 1s - loss: 0.0341 - 755ms/epoch - 50ms/step\nEpoch 17/20\n15/15 - 1s - loss: 0.0308 - 779ms/epoch - 52ms/step\nEpoch 18/20\n15/15 - 1s - loss: 0.0279 - 774ms/epoch - 52ms/step\nEpoch 19/20\n15/15 - 1s - loss: 0.0258 - 792ms/epoch - 53ms/step\nEpoch 20/20\n15/15 - 1s - loss: 0.0229 - 773ms/epoch - 52ms/step\nModel: \"sequential_3\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n simple_rnn_2 (SimpleRNN)    (None, 3)                 15        \n                                                                 \n dense_3 (Dense)             (None, 1)                 4         \n                                                                 \n=================================================================\nTotal params: 19\nTrainable params: 19\nNon-trainable params: 0\n_________________________________________________________________\n1/1 [==============================] - 0s 50ms/step\n1/1 [==============================] - 0s 45ms/step\nTrain MSE: 0.022 MSE\nTrain MSE: 0.187 MSE\nTrain RMSE: 0.148 RMSE\nTest RMSE: 0.433 RMSE\n\n\n\n\n\n\n\n\n\nmodel = create_LSTM()\nmodel.fit(trainX, trainY, epochs=20, batch_size=1, verbose=2)\nmodel.summary()\n\n# make predictions\ntrain_predict = model.predict(trainX)\ntest_predict = model.predict(testX)\n# Mean square error\nprint_error(trainY, testY, train_predict, test_predict)\n# plot the result [Actual and Predicted Values]\nplot_result(trainY, testY, train_predict, test_predict)\n\nEpoch 1/20\n15/15 - 3s - loss: 0.2534 - 3s/epoch - 167ms/step\nEpoch 2/20\n15/15 - 2s - loss: 0.1712 - 2s/epoch - 113ms/step\nEpoch 3/20\n15/15 - 2s - loss: 0.1654 - 2s/epoch - 111ms/step\nEpoch 4/20\n15/15 - 2s - loss: 0.1491 - 2s/epoch - 109ms/step\nEpoch 5/20\n15/15 - 2s - loss: 0.1559 - 2s/epoch - 110ms/step\nEpoch 6/20\n15/15 - 3s - loss: 0.1385 - 3s/epoch - 211ms/step\nEpoch 7/20\n15/15 - 3s - loss: 0.1075 - 3s/epoch - 208ms/step\nEpoch 8/20\n15/15 - 2s - loss: 0.1158 - 2s/epoch - 111ms/step\nEpoch 9/20\n15/15 - 2s - loss: 0.1181 - 2s/epoch - 107ms/step\nEpoch 10/20\n15/15 - 2s - loss: 0.1057 - 2s/epoch - 111ms/step\nEpoch 11/20\n15/15 - 2s - loss: 0.1118 - 2s/epoch - 109ms/step\nEpoch 12/20\n15/15 - 2s - loss: 0.1441 - 2s/epoch - 108ms/step\nEpoch 13/20\n15/15 - 2s - loss: 0.1211 - 2s/epoch - 153ms/step\nEpoch 14/20\n15/15 - 2s - loss: 0.1076 - 2s/epoch - 121ms/step\nEpoch 15/20\n15/15 - 2s - loss: 0.1072 - 2s/epoch - 110ms/step\nEpoch 16/20\n15/15 - 2s - loss: 0.1043 - 2s/epoch - 116ms/step\nEpoch 17/20\n15/15 - 2s - loss: 0.1103 - 2s/epoch - 109ms/step\nEpoch 18/20\n15/15 - 2s - loss: 0.1189 - 2s/epoch - 110ms/step\nEpoch 19/20\n15/15 - 2s - loss: 0.1227 - 2s/epoch - 110ms/step\nEpoch 20/20\n15/15 - 2s - loss: 0.1120 - 2s/epoch - 145ms/step\nModel: \"sequential_4\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n bidirectional_1 (Bidirectio  (1, 64)                  8704      \n nal)                                                            \n                                                                 \n dense_4 (Dense)             (1, 1)                    65        \n                                                                 \n=================================================================\nTotal params: 8,769\nTrainable params: 8,769\nNon-trainable params: 0\n_________________________________________________________________\n1/1 [==============================] - 1s 654ms/step\n1/1 [==============================] - 0s 66ms/step\nTrain MSE: 0.017 MSE\nTrain MSE: 0.176 MSE\nTrain RMSE: 0.131 RMSE\nTest RMSE: 0.419 RMSE\n\n\n\n\n\n\n\n\n\nmodel = create_GRU()\nmodel.fit(trainX, trainY, epochs=20, batch_size=1, verbose=2)\nmodel.summary()\n\n# make predictions\ntrain_predict = model.predict(trainX)\ntest_predict = model.predict(testX)\n# Mean square error\nprint_error(trainY, testY, train_predict, test_predict)\n# plot the result [Actual and Predicted Values]\nplot_result(trainY, testY, train_predict, test_predict)\n\nEpoch 1/20\n15/15 - 7s - loss: 0.2110 - 7s/epoch - 452ms/step\nEpoch 2/20\n15/15 - 2s - loss: 0.1669 - 2s/epoch - 166ms/step\nEpoch 3/20\n15/15 - 2s - loss: 0.1516 - 2s/epoch - 159ms/step\nEpoch 4/20\n15/15 - 3s - loss: 0.1390 - 3s/epoch - 176ms/step\nEpoch 5/20\n15/15 - 2s - loss: 0.1263 - 2s/epoch - 128ms/step\nEpoch 6/20\n15/15 - 2s - loss: 0.1122 - 2s/epoch - 161ms/step\nEpoch 7/20\n15/15 - 4s - loss: 0.1029 - 4s/epoch - 253ms/step\nEpoch 8/20\n15/15 - 3s - loss: 0.0959 - 3s/epoch - 216ms/step\nEpoch 9/20\n15/15 - 2s - loss: 0.0912 - 2s/epoch - 164ms/step\nEpoch 10/20\n15/15 - 2s - loss: 0.0931 - 2s/epoch - 131ms/step\nEpoch 11/20\n15/15 - 2s - loss: 0.0864 - 2s/epoch - 130ms/step\nEpoch 12/20\n15/15 - 3s - loss: 0.0870 - 3s/epoch - 178ms/step\nEpoch 13/20\n15/15 - 3s - loss: 0.0921 - 3s/epoch - 201ms/step\nEpoch 14/20\n15/15 - 3s - loss: 0.0865 - 3s/epoch - 170ms/step\nEpoch 15/20\n15/15 - 2s - loss: 0.0792 - 2s/epoch - 129ms/step\nEpoch 16/20\n15/15 - 2s - loss: 0.0815 - 2s/epoch - 130ms/step\nEpoch 17/20\n15/15 - 2s - loss: 0.0822 - 2s/epoch - 130ms/step\nEpoch 18/20\n15/15 - 2s - loss: 0.0747 - 2s/epoch - 130ms/step\nEpoch 19/20\n15/15 - 2s - loss: 0.0721 - 2s/epoch - 146ms/step\nEpoch 20/20\n15/15 - 3s - loss: 0.0704 - 3s/epoch - 233ms/step\nModel: \"sequential_5\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n bidirectional_2 (Bidirectio  (1, 64)                  6720      \n nal)                                                            \n                                                                 \n dense_5 (Dense)             (1, 1)                    65        \n                                                                 \n=================================================================\nTotal params: 6,785\nTrainable params: 6,785\nNon-trainable params: 0\n_________________________________________________________________\n1/1 [==============================] - 1s 605ms/step\n1/1 [==============================] - 0s 79ms/step\nTrain MSE: 0.007 MSE\nTrain MSE: 0.094 MSE\nTrain RMSE: 0.084 RMSE\nTest RMSE: 0.307 RMSE\n\n\n\n\n\n\n\n\nAmoung those 3 ANN methods, GRU is more accurate in the test set.\nIncluding regularization in an ANN model has the effect of constraining the weights of the model, which helps to prevent overfitting of the model to the training data. Overfitting occurs when a model is overly complex and captures noise in the training data rather than the underlying patterns. Regularization techniques such as L1 and L2 regularization penalize large weights, which can help prevent overfitting by encouraging the model to focus on the most important features in the data.\nThe accuracy of a deep learning model in predicting the future depends on various factors such as the quality and quantity of data, the complexity of the model, and the specific problem being solved. In general, deep learning models are known to be good at capturing complex relationships between input and output variables, which can make them well-suited for time-series forecasting tasks. However, the accuracy of the model may decrease as the prediction horizon increases, as the model is forced to make predictions further into the future based on less information. The optimal prediction horizon for a deep learning model depends on the specific problem and the available data, but typically ranges from a few time steps to a few months or years.\nDeep learning models is more effective in capturing complex relationships and patterns in data, particularly when dealing with large and complex datasets. This can make them particularly useful for time-series forecasting tasks that involve multiple variables, nonlinear relationships, and complex dynamics.\n\n\n\nIn terms of forecasting performance, the deep learning models generally outperformed the traditional time series models (ARMA/ARIMA/SARIMA, ARIMAX/SARIMAX/VAR) in this project, as measured by the RMSE metric. This suggests that the deep learning models were better able to capture the complex and non-linear relationships between the input variables and the target variable, and were able to make more accurate predictions as a result.\nThe ARMA/ARIMA/SARIMA models had relatively high RMSE values, indicating that they were not able to capture all of the underlying patterns and trends in the data. The addition of exogenous variables through ARIMAX/SARIMAX models improved their performance slightly, but the RMSE values were still relatively high compared to the deep learning models.\nWhile the VAR models performed better than the ARMA/ARIMA/SARIMA models, their RMSE values were still higher than those of the deep learning models. This suggests that the deep learning models were better able to capture the complex relationships between the input variables and the target variable, and were able to make more accurate predictions as a result.\nOverall, the deep learning models showed superior performance in this project compared to the traditional time series models. However, it is worth noting that the deep learning models are more complex and computationally intensive, and may not be the best choice for all forecasting tasks depending on the available data and the specific problem at hand."
  },
  {
    "objectID": "models.html#stationarity",
    "href": "models.html#stationarity",
    "title": "ARMA/ARIMA/SARIMA Models",
    "section": "Stationarity",
    "text": "Stationarity\n\n\n\n\n\n\n\n\nFirst difference works well for my data set, the second and third difference may cause over differencing.\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  diff(emp_ts)\nDickey-Fuller = -5.1868, Lag order = 6, p-value = 0.01\nalternative hypothesis: stationary\n\n\nP-value is smaller than 0.05. We have enough evidence to reject the null hypothesis at 5% significance level. So the ADF test thought the series is stationary."
  },
  {
    "objectID": "models.html#acfpacf",
    "href": "models.html#acfpacf",
    "title": "ARMA/ARIMA/SARIMA Models",
    "section": "ACF/PACF",
    "text": "ACF/PACF\n\n\n\n\n\np = 1; q = 1"
  },
  {
    "objectID": "models.html#arima-model-fitting",
    "href": "models.html#arima-model-fitting",
    "title": "ARMA/ARIMA/SARIMA Models",
    "section": "ARIMA Model Fitting",
    "text": "ARIMA Model Fitting\n\n\n\n\n\np\nd\nq\nAIC\nBIC\nAICc\n\n\n\n\n1\n1\n1\n-1666.538\n-1656.097\n-1666.437\n\n\n1\n1\n2\n-1665.441\n-1651.519\n-1665.271\n\n\n1\n1\n3\n-1664.501\n-1647.097\n-1664.244\n\n\n2\n1\n1\n-1665.303\n-1651.380\n-1665.133\n\n\n2\n1\n2\n-1663.992\n-1646.589\n-1663.735\n\n\n2\n1\n3\n-1665.338\n-1644.454\n-1664.978\n\n\n3\n1\n1\n-1664.659\n-1647.256\n-1664.403\n\n\n3\n1\n2\n-1664.962\n-1644.078\n-1664.602\n\n\n3\n1\n3\n-1663.348\n-1638.984\n-1662.865\n\n\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\n\n\n\n\n  p d q       AIC       BIC      AICc\n1 1 1 1 -1666.538 -1656.097 -1666.437\n\n\n  p d q       AIC       BIC      AICc\n1 1 1 1 -1666.538 -1656.097 -1666.437\n\n\n  p d q       AIC       BIC      AICc\n1 1 1 1 -1666.538 -1656.097 -1666.437\n\n\nAIC, BIC, AICc suggest ARIMA(1, 1, 1) is the best model.\n\n\nSeries: emp_ts \nARIMA(1,1,1) \n\nCoefficients:\n         ar1      ma1\n      0.5917  -0.4420\ns.e.  0.3558   0.4005\n\nsigma^2 = 5.58e-05:  log likelihood = 836.27\nAIC=-1666.54   AICc=-1666.44   BIC=-1656.1\n\nTraining set error measures:\n                        ME        RMSE         MAE          MPE       MAPE\nTraining set -8.981908e-05 0.007423551 0.003284021 -0.001143603 0.04134745\n                  MASE       ACF1\nTraining set 0.1359915 0.01066391\n\n\nEquation:\n\\[x_t = 2.24x_{t-1} - 0.73x_{t-2} - 1.27x_{t-3} + 0.76x_{t-4} + w_t - 2.96w_{t-1} + 2.96w_{t-2} - w_{t-3}\\]"
  },
  {
    "objectID": "models.html#model-diagnostic",
    "href": "models.html#model-diagnostic",
    "title": "ARMA/ARIMA/SARIMA Models",
    "section": "Model Diagnostic",
    "text": "Model Diagnostic\n\n\n\n\n\nFrom the model diagnostic chart above, we can observe that the standardized residuals are similar to white noise, except there’s a huge drop in employment during 2020. Also, both the p-value for Ljung-Box statistics and the ACF of residuals all falls within the confidence interval. We could say that the ARIMA(1, 1, 1) model fits the data set well!"
  },
  {
    "objectID": "models.html#auto.arima",
    "href": "models.html#auto.arima",
    "title": "ARMA/ARIMA/SARIMA Models",
    "section": "auto.arima",
    "text": "auto.arima\n\n\nSeries: emp_ts \nARIMA(0,2,2) \n\nCoefficients:\n          ma1      ma2\n      -0.8254  -0.1473\ns.e.   0.0648   0.0646\n\nsigma^2 = 5.644e-05:  log likelihood = 830.64\nAIC=-1655.29   AICc=-1655.19   BIC=-1644.86\n\n\nThe result from auto.arima function is different. However, I’m pretty condident about my model above since AIC, AICc, and BIC have a relatively low value comparing to the model suggested by auto.arima function."
  },
  {
    "objectID": "models.html#forecast",
    "href": "models.html#forecast",
    "title": "ARMA/ARIMA/SARIMA Models",
    "section": "Forecast",
    "text": "Forecast\n\n\n\n\n\nwe get a reasonable forecast, however, we should try SARIMA model as well."
  },
  {
    "objectID": "models.html#benchmark-method-comparison",
    "href": "models.html#benchmark-method-comparison",
    "title": "ARMA/ARIMA/SARIMA Models",
    "section": "Benchmark Method Comparison",
    "text": "Benchmark Method Comparison\n\n\n\n\n\n\n\n                        ME        RMSE         MAE          MPE       MAPE\nTraining set -8.981908e-05 0.007423551 0.003284021 -0.001143603 0.04134745\n                  MASE       ACF1\nTraining set 0.1359915 0.01066391\n\n\n                       ME       RMSE        MAE          MPE     MAPE    MASE\nTraining set -1.01924e-16 0.05594163 0.04929746 -0.004927857 0.618662 2.04141\n                  ACF1\nTraining set 0.9780381\n\n\n                        ME        RMSE         MAE          MPE       MAPE\nTraining set -0.0001689599 0.007548654 0.003509744 -0.002142445 0.04417974\n                  MASE      ACF1\nTraining set 0.1453387 0.1739657\n\n\n                       ME       RMSE        MAE         MPE      MAPE MASE\nTraining set -0.001233452 0.03526145 0.02414872 -0.01630537 0.3034857    1\n                  ACF1\nTraining set 0.9514107\n\n\n                        ME        RMSE         MAE          MPE       MAPE\nTraining set -0.0001689599 0.007548654 0.003509744 -0.002142445 0.04417974\n                  MASE      ACF1\nTraining set 0.1453387 0.1739657\n\n\nBoth the viz and accuracy metric suggest my model is better than those benchmark methods."
  },
  {
    "objectID": "models.html#sarima-model-fitting",
    "href": "models.html#sarima-model-fitting",
    "title": "ARMA/ARIMA/SARIMA Models",
    "section": "SARIMA Model Fitting",
    "text": "SARIMA Model Fitting\nSince the employment data set I’m using doesn’t not contain seasonality as it’s seasonal adjusted. So, I would use real interest rate data set as the source data for SARIMA model since interest rate is related to the employment.\n\n\n\n\nir_ts %>%\n  decompose(type = c(\"additive\", \"multiplicative\")) %>%\n  autoplot()\n\n\n\n\n\n\n\n\n\nAfter ordinary and seasonal differencing, we can tell that p should be 0, d should be 1, q should be 0, P should be 1, D should be 1, Q should be 1, 2, 3\n\n\n   p d q P D Q       AIC        BIC      AICc\n1  0 1 0 1 1 1 -32.91451 -20.380674 -32.86430\n2  0 1 0 1 1 2 -30.96887 -14.257090 -30.88501\n3  0 1 0 1 1 3 -28.90980  -8.020084 -28.78375\n4  0 1 0 2 1 1 -30.93132 -14.219544 -30.84746\n5  0 1 0 2 1 2 -28.91719  -8.027466 -28.79114\n6  0 1 1 1 1 1 -31.63568 -14.923904 -31.55182\n7  0 1 1 1 1 2 -29.70612  -8.816396 -29.58007\n8  0 1 1 2 1 1 -29.65553  -8.765810 -29.52948\n9  0 1 2 1 1 1 -31.01094 -10.121217 -30.88489\n10 1 1 0 1 1 1 -31.55578 -14.844000 -31.47192\n11 1 1 0 1 1 2 -29.62690  -8.737177 -29.50085\n12 1 1 0 2 1 1 -29.57622  -8.686497 -29.45017\n13 1 1 1 1 1 1 -29.06174  -8.172018 -28.93569\n14 2 1 0 1 1 1 -31.09532 -10.205603 -30.96927\n\n\n\n\n  p d q P D Q       AIC       BIC     AICc\n1 0 1 0 1 1 1 -32.91451 -20.38067 -32.8643\n\n\n  p d q P D Q       AIC       BIC     AICc\n1 0 1 0 1 1 1 -32.91451 -20.38067 -32.8643\n\n\n  p d q P D Q       AIC       BIC     AICc\n1 0 1 0 1 1 1 -32.91451 -20.38067 -32.8643\n\n\nThe AIC, BIC, AICc scores all suggests SARIMA(0, 1, 0) * (1, 1, 1) model.\n\n\nSeries: ir_ts \nARIMA(0,1,0)(1,1,1)[12] \n\nCoefficients:\n         sar1     sma1\n      -0.0354  -0.9385\ns.e.   0.0513   0.0288\n\nsigma^2 = 0.05136:  log likelihood = 19.46\nAIC=-32.91   AICc=-32.86   BIC=-20.38\n\nTraining set error measures:\n                     ME      RMSE     MAE      MPE     MAPE      MASE\nTraining set 0.01865717 0.2231673 0.16551 3.801952 29.13172 0.2955925\n                    ACF1\nTraining set -0.04271696"
  },
  {
    "objectID": "models.html#forecast-1",
    "href": "models.html#forecast-1",
    "title": "ARMA/ARIMA/SARIMA Models",
    "section": "Forecast",
    "text": "Forecast"
  },
  {
    "objectID": "models.html#benchmark-method-comparison-1",
    "href": "models.html#benchmark-method-comparison-1",
    "title": "ARMA/ARIMA/SARIMA Models",
    "section": "Benchmark Method Comparison",
    "text": "Benchmark Method Comparison\n\n\n\n\n\n\n\n                     ME      RMSE     MAE      MPE     MAPE      MASE\nTraining set 0.01865717 0.2231673 0.16551 3.801952 29.13172 0.2955925\n                    ACF1\nTraining set -0.04271696\n\n\n                       ME    RMSE      MAE      MPE     MAPE     MASE      ACF1\nTraining set 2.580427e-16 1.86868 1.543425 101.5844 567.5626 2.756479 0.9851519\n\n\n                      ME      RMSE       MAE      MPE     MAPE      MASE\nTraining set -0.01125957 0.2227021 0.1684397 2.277912 27.82212 0.3008247\n                    ACF1\nTraining set -0.02748644\n\n\n                     ME      RMSE       MAE      MPE     MAPE MASE      ACF1\nTraining set -0.1373641 0.7155465 0.5599264 8.369501 87.15671    1 0.8880544\n\n\n                      ME      RMSE       MAE      MPE     MAPE      MASE\nTraining set -0.01125957 0.2227021 0.1684397 2.277912 27.82212 0.3008247\n                    ACF1\nTraining set -0.02748644\n\n\nBoth the viz and accuracy metric suggest my model is better than those benchmark methods."
  },
  {
    "objectID": "models.html#cross-validation",
    "href": "models.html#cross-validation",
    "title": "ARMA/ARIMA/SARIMA Models",
    "section": "Cross Validation",
    "text": "Cross Validation\n\n1 step ahead\n\n\n              Jan          Feb          Mar          Apr          May\n1982  5.583287521  5.055632821  5.334986621  5.246680361  5.272018091\n1983  3.788344321  3.500477921  3.626761891  3.329032981  3.736197691\n1984  4.027657551  4.389218821  4.555388191  4.778104361  5.272407781\n1985  3.942457281  4.310475261  4.118027491  3.864126011  3.082644751\n1986  2.550519861  2.135821501  1.874944361  1.973229771  2.511123261\n1987  1.505220801  1.481424901  1.686838621  2.286353851  2.259052111\n1988  2.025008581  1.969377511  2.343195931  2.422911201  2.577520131\n1989  2.658519271  2.900361721  2.797689491  2.556301941  2.337805091\n1990  2.293566841  2.478607071  2.478801411  2.731701211  2.309250601\n1991  1.833693271  2.102475631  2.036472551  1.976555761  2.114283711\n1992  1.554931041  1.586904431  1.716499321  1.672809421  1.495216111\n1993  0.931424691  0.600064381  0.701156381  0.591360071  0.687876791\n1994  0.546808201  0.897842501  1.585822231  1.551216541  1.520204971\n1995  1.981636181  1.647625021  1.581566751  1.347120251  0.831352321\n1996  0.524498171  0.823933051  1.034973871  1.231665321  1.420719741\n1997  1.140014341  1.235195251  1.477112821  1.363620621  1.205747771\n1998  0.703708361  0.779151411  0.772704981  0.819681731  0.797636901\n1999  0.294229641  0.636998261  0.604429381  0.630877501  1.043202561\n2000  1.439982841  1.366986811  1.129606101  1.287170551  1.054532351\n2001  0.193653431  0.193934181  0.147623361  0.360057421  0.453095471\n2002  0.326712881  0.347929631  0.627765771  0.277189381  0.246503361\n2003 -0.606981549 -0.849769599 -0.763742079 -0.702639569 -1.168244809\n2004 -0.501779809 -0.603195889 -0.715211139 -0.190115389 -0.126784439\n2005 -0.260201429 -0.094019439 -0.052833709 -0.283494869 -0.406472439\n2006  0.131052231  0.310260331  0.285032481  0.333009601  0.377449311\n2007  0.213790361 -0.048374149  0.004126301 -0.041516349  0.172949181\n2008 -0.962237689 -1.048140469 -1.010466329 -0.672844259 -0.585806539\n2009 -1.034054059 -0.822618779 -1.360540879 -1.055949649 -0.751255009\n2010 -0.948845139 -1.074629019 -0.764255249 -0.846972689 -1.053999569\n2011 -1.243673369 -1.199509669 -1.095857719 -1.361885709 -1.581093559\n2012 -2.051509519 -1.904123639 -1.878714819 -2.160582529 -2.121511599\n2013 -1.901260369 -2.023019829 -2.097572429 -2.092751779 -1.845666799\n2014 -1.564683239 -1.492907369 -1.371138449 -1.446506399 -1.448831109\n2015 -1.534980169 -1.321655129 -1.663038029 -1.539439109 -1.519582749\n2016 -1.534622549 -1.521953129 -1.728087869 -1.659710719 -1.554262029\n2017 -1.298978969 -1.156857529 -1.307606769 -1.296951749 -1.337249199\n2018 -0.984891259 -1.043824719 -1.059006219 -0.857268749 -0.827632409\n2019 -1.005177699 -1.556152899 -1.653056799 -1.532931749 -1.652058929\n2020 -1.757374759 -2.201197999 -2.184954969 -2.101217069 -2.227059909\n2021 -2.304184349 -2.060853679 -1.955313609 -2.036412969 -2.079025399\n2022 -1.631767659 -1.726270629 -1.123960279 -0.869462789 -1.048267039\n2023 -0.653158379 -0.011847659           NA                          \n              Jun          Jul          Aug          Sep          Oct\n1982  5.485626071  4.806437591  4.433864341  4.205801631  3.552853681\n1983  3.745218551  4.196061231  4.281664301  3.959332781  4.070577371\n1984  5.372783731  4.797788121  4.902293601  4.535492711  3.993120891\n1985  3.260567801  3.516757591  3.349913551  3.334334741  3.169130251\n1986  1.919667301  1.847220151  1.478365871  1.793694041  1.670525841\n1987  2.204168071  2.406871211  2.530839661  2.885881261  2.452071161\n1988  2.361992201  2.469384271  2.634499701  2.388633361  2.285361631\n1989  1.929315681  1.665992181  2.203710881  2.223822781  1.956774401\n1990  2.268457951  2.097171521  2.325384151  2.127178471  1.985494431\n1991  2.148893201  2.066556841  1.805135001  1.564501511  1.491697171\n1992  1.279738891  1.051462051  0.883981811  0.654683201  1.251272791\n1993  0.518438231  0.629542521  0.377220141  0.325482641  0.488934781\n1994  1.713058851  1.506923501  1.563743951  1.883262521  2.029892201\n1995  0.864669611  1.075127891  0.944238011  0.936368851  0.810863861\n1996  1.295900001  1.256054831  1.447461541  1.244988921  1.068286201\n1997  1.126515871  1.015151351  1.077508971  0.943230191  0.899596281\n1998  0.697345161  0.705979631  0.422878661 -0.138215079  0.147669931\n1999  0.932201251  1.028182761  1.096104881  1.069561311  1.073892981\n2000  0.988994301  0.972252731  0.812635631  0.803265041  0.759777381\n2001  0.455006891  0.261000871  0.104407421 -0.086428099 -0.258595019\n2002  0.068568001 -0.331097959 -0.627252299 -0.804664909 -0.677773739\n2003 -0.944671589 -0.321656089 -0.176122299 -0.692541189 -0.359458899\n2004 -0.139561929 -0.217557859 -0.588668909 -0.386254249 -0.465671789\n2005 -0.253339469 -0.065143779 -0.337670379 -0.106315709  0.002328461\n2006  0.422988981  0.224925551  0.132696641  0.138701521  0.126524851\n2007  0.300212871  0.060520221 -0.127137969 -0.084246329 -0.247113429\n2008 -0.627779459 -0.716863219 -0.833785829 -0.588297239 -0.441732009\n2009 -0.806457729 -0.914320179 -1.124122909 -1.076219389 -1.011950509\n2010 -1.226601479 -1.417196659 -1.496292799 -1.664555299 -1.734470989\n2011 -1.499408199 -1.678385639 -1.858421669 -1.974798059 -1.936291019\n2012 -2.128186579 -2.133445329 -2.242617689 -2.166532789 -2.079097599\n2013 -1.526979509 -1.598291859 -1.532526039 -1.580391129 -1.583722809\n2014 -1.482365599 -1.423954849 -1.492500619 -1.372395849 -1.360060289\n2015 -1.406285649 -1.525054989 -1.560900639 -1.544588839 -1.407154849\n2016 -1.908013609 -1.846523089 -1.857286249 -1.783563739 -1.726751219\n2017 -1.348344429 -1.402500389 -1.508147909 -1.344217039 -1.321170099\n2018 -0.918126839 -0.818118929 -0.941213649 -0.773233739 -0.764702199\n2019 -1.632788669 -1.653308759 -1.838355029 -1.705036019 -1.639748699\n2020 -2.418705049 -2.480494549 -2.416697479 -2.365098999 -2.287316189\n2021 -2.038319469 -2.334379339 -2.268091419 -1.985459409 -1.909362669\n2022 -1.011185619 -1.217735249 -0.869111869 -0.276028519 -0.142647499\n2023                                                                 \n              Nov          Dec\n1982  3.808370871  3.470028951\n1983  4.075063991  4.141943941\n1984  4.002264561  4.088540021\n1985  3.007408971  2.473824711\n1986  1.470373811  1.531205181\n1987  2.505914961  2.532025011\n1988  2.544578171  2.789833161\n1989  1.956305281  2.054262851\n1990  1.820560151  1.849570601\n1991  1.336972911  1.125181901\n1992  1.436324881  1.194547911\n1993  0.646755921  0.699324381\n1994  2.043592521  2.170929671\n1995  0.648792911  0.574231421\n1996  0.877574561  1.181507251\n1997  0.846490491  0.766941081\n1998  0.156219261  0.234791901\n1999  1.118842951  1.337690571\n2000  0.565849871  0.178072391\n2001 -0.032784249  0.450139411\n2002 -0.381106889 -0.585158809\n2003 -0.359787889 -0.371406189\n2004 -0.163823929 -0.294084649\n2005 -0.055013279 -0.008518829\n2006  0.048777231  0.074715241\n2007 -0.686391739 -0.673869869\n2008 -1.097980699 -1.134819509\n2009 -1.225319989 -0.775435689\n2010 -1.447151279 -1.209383389\n2011 -1.928892239 -1.927260039\n2012 -2.245737999 -1.988959129\n2013 -1.655060619 -1.392723839\n2014 -1.688635179 -1.363417749\n2015 -1.425938319 -1.338854579\n2016 -1.372903969 -1.347709479\n2017 -1.500311319 -1.199572539\n2018 -0.818393889 -1.032926209\n2019 -1.559095619 -1.568264239\n2020 -2.305682009 -2.308377759\n2021 -1.981671379 -1.753865449\n2022 -0.497235399 -0.290261579\n2023                          \n\n\nMAE\n\n\n[1] 0.3754496\n\n\n\n\n12 steps ahead"
  },
  {
    "objectID": "financial.html",
    "href": "financial.html",
    "title": "Financial Time Series Models",
    "section": "",
    "text": "Because the p-value is less than 0.05, meaning that we need some other model to fit it since there is ARCH effect in the residuals for the ARMA model.\nBecause the AIC and BIC of fit2 is lower than that of fit1, so we say the second model performs better, which is arma(1,0) + garch(1,1)."
  },
  {
    "objectID": "financial.html#summary",
    "href": "financial.html#summary",
    "title": "Financial Time Series Models",
    "section": "Summary",
    "text": "Summary\nAfter analysis, we fitted an ARMA(1,0)+GARCH(1,1) model for the returns of JPM stock. According to Ljung-Box test, all p-value are greater than 0.05, meaning that there is no correlation left in the residuals so we can say that the model fitted well.\nϕ(B)x_t=δ+θ(B)y_t\nϕ(B)=1−0.003B\nvar(y_t|y_t−1) = σ^2 = 4.19×10^-5 + 0.16(y_{t−1})^2 + 0.74σ^2_{t_1}"
  },
  {
    "objectID": "sources.html",
    "href": "sources.html",
    "title": "Data Sources",
    "section": "",
    "text": "The U.S. Bureau of Labor Statistics (BLS) is a federal government agency that is responsible for collecting, analyzing, and disseminating information on labor market activity, working conditions, and price changes in the U.S. economy. The agency’s mission is to promote and improve the economic well-being of the American people by providing accurate and timely information about the labor market and economy.\n\n\n\nDownload Data Here!"
  },
  {
    "objectID": "sources.html#u.s.-bureau-of-economic-analysis---gdp-quarterly",
    "href": "sources.html#u.s.-bureau-of-economic-analysis---gdp-quarterly",
    "title": "Data Sources",
    "section": "U.S. Bureau of Economic Analysis - GDP (Quarterly):",
    "text": "U.S. Bureau of Economic Analysis - GDP (Quarterly):\nThe U.S. Bureau of Economic Analysis (BEA) is a federal government agency that provides essential data and analysis on the performance and structure of the U.S. economy. The BEA’s mission is to promote a better understanding of the U.S. economy by providing timely, relevant, and accurate economic accounts data to policymakers, businesses, and the public.\n\n\n\nDownload Data Here!"
  },
  {
    "objectID": "sources.html#u.s.-bureau-of-economic-analysis---interest-rate-monthly",
    "href": "sources.html#u.s.-bureau-of-economic-analysis---interest-rate-monthly",
    "title": "Data Sources",
    "section": "U.S. Bureau of Economic Analysis - Interest Rate (Monthly):",
    "text": "U.S. Bureau of Economic Analysis - Interest Rate (Monthly):\nInterest rates are a fundamental component of the global financial system, playing a crucial role in determining the cost of borrowing and the returns on investments. An interest rate is the amount charged, expressed as a percentage of the principal, by a lender to a borrower for the use of their funds. Interest rates can be set by central banks, commercial banks, or other financial institutions, and can vary depending on a range of factors, such as the state of the economy, inflation, and risk.\n\n\n\nDownload Data Here!"
  },
  {
    "objectID": "conclusions.html",
    "href": "conclusions.html",
    "title": "Conclusions",
    "section": "",
    "text": "In conclusion, our time series analysis of US employment using factors including GDP and interest rates revealed several key insights.\nFirstly, we found that employment in the US has generally followed a positive trend over the period analyzed, with some periods of significant growth and others of decline. The financial crisis of 2008-2009 had a significant impact on employment, leading to job losses across many industries.\nSecondly, we found that GDP has a significant impact on employment, with positive GDP growth generally associated with increases in employment. However, the relationship between GDP and employment is complex and can be influenced by a range of other factors, such as technological change and shifts in consumer behavior.\nFinally, we found that interest rates also play an important role in employment, with lower interest rates generally associated with increased employment. This suggests that monetary policy can be an important tool for promoting employment growth in the US.\nOverall, our analysis highlights the importance of considering multiple factors when analyzing employment trends in the US, including both macroeconomic factors such as GDP and interest rates and sector-specific factors such as industry trends and technological change. By taking a comprehensive approach to analyzing employment trends, policymakers and businesses can gain a deeper understanding of the drivers of employment growth and develop strategies to promote long-term job growth and economic stability."
  },
  {
    "objectID": "introduction.html",
    "href": "introduction.html",
    "title": "Introduction",
    "section": "",
    "text": "Employment trends in the United States are a crucial aspect of the country’s economic and social landscape. The complex and dynamic nature of employment data necessitates a sophisticated approach to data analysis, and time series analysis is a powerful tool that can provide valuable insights into historical trends and future projections. In this research project, we aim to apply time series analysis techniques to United States employment data, with the goal of identifying key patterns, trends, and relationships between various economic and demographic factors.\nThe scope of this research project encompasses a wide range of employment-related data, including job creation and job loss across various industries, demographic factors such as age, gender, and education level, and the overall unemployment rate. The research project will leverage a variety of statistical techniques and models, including regression analysis, time series decomposition, and forecasting methods.\nThe insights gained from this research project could have significant implications for policymakers, employers, and job seekers. By understanding historical employment trends and predicting future employment patterns, policymakers can develop more effective policies to promote job growth and reduce unemployment. Employers can use this information to anticipate changes in the labor market and adapt their hiring practices accordingly, while job seekers can make more informed decisions about their education and career paths.\nOverall, this research project will provide a comprehensive analysis of United States employment trends, and will contribute to our understanding of the complex dynamics of the American labor market. By applying sophisticated time series analysis techniques to this rich and diverse dataset, we aim to shed new light on the challenges and opportunities facing the United States economy in the coming years.\n\n\n\nWhat are the historical trends in job creation and job loss across different industries in the United States, and how have these trends changed over time?\nHow do demographic factors such as age, gender, and education level impact employment rates in the United States, and what are the long-term trends in these relationships?\nWhat factors contribute to fluctuations in the unemployment rate in the United States, and how can policymakers use this information to create more stable employment conditions?\nHow can we forecast future employment trends in the United States, based on historical data and demographic factors?\nWhat are the key economic indicators that are most strongly correlated with changes in employment rates in the United States, and how have these relationships evolved over time?\nHow does the seasonal nature of employment trends in certain industries impact the overall labor market in the United States, and what are the most effective strategies for managing these seasonal fluctuations?\nWhat are the long-term trends in job polarization and income inequality in the United States, and how do these trends impact different demographic groups?\nHow can we use machine learning and artificial intelligence techniques to enhance our understanding of employment trends in the United States, and to develop more accurate forecasting models?\nWhat is the impact of technological change on employment trends in the United States, and how can policymakers and employers respond to these changes to promote job growth and stability?\nWhat are the most effective strategies for promoting workforce development and training in the United States, and how can we use time series analysis to evaluate the effectiveness of different training programs and initiatives?"
  },
  {
    "objectID": "arimax.html#literature-review",
    "href": "arimax.html#literature-review",
    "title": "SARIMAX",
    "section": "Literature Review",
    "text": "Literature Review\nThe followings are quantitative influential factors of the US employment:\n\nGross Domestic Product (GDP): As mentioned earlier, GDP is a key indicator of economic growth and is strongly correlated with employment levels. A higher GDP generally indicates a stronger economy, which can create new job opportunities as businesses expand and invest in new projects.\nUnemployment rate: The unemployment rate measures the percentage of the labor force that is currently unemployed. A higher unemployment rate indicates a weaker job market, while a lower unemployment rate suggests a stronger job market. However, it is important to note that the unemployment rate can be influenced by factors such as labor force participation rates, which can complicate its interpretation.\nLabor force participation rate: The labor force participation rate measures the percentage of the working-age population that is either employed or actively seeking employment. A higher labor force participation rate generally indicates a stronger job market, as more people are actively seeking work.\nConsumer spending: Consumer spending is an important driver of economic growth, and can create job opportunities in industries such as retail and hospitality. A higher level of consumer spending generally indicates a stronger economy, which can create new job opportunities.\nBusiness investment: Business investment is another important driver of economic growth, as it can create new job opportunities in industries such as manufacturing and construction. A higher level of business investment generally indicates a stronger economy, which can lead to increased employment levels.\nTrade balance: The trade balance measures the difference between the value of a country’s exports and imports. A trade deficit (where imports exceed exports) can create job losses in certain industries, while a trade surplus (where exports exceed imports) can create job gains in certain industries.\nInterest rates: Interest rates can influence the availability of credit for businesses and individuals, which can impact investment and spending decisions. Higher interest rates can make borrowing more expensive, which can reduce business investment and consumer spending, potentially leading to job losses. Conversely, lower interest rates can stimulate borrowing, investment, and spending, potentially creating new job opportunities.\n\nAfter literature review of what quantitative factor would influence the US employment, I decided to use interest rates and GDP as exogenous variables and the employment as response variable to fit an ARIMAX model."
  },
  {
    "objectID": "arimax.html#plot-the-original-data",
    "href": "arimax.html#plot-the-original-data",
    "title": "SARIMAX",
    "section": "Plot the Original Data",
    "text": "Plot the Original Data\n\n\n\n\n\n\n\n\nDATE\nGDP\nInformation Employment\nInterest Rate\n\n\n\n\n2003-04-01\n11312.77\n3208\n1.309619\n\n\n2003-07-01\n11566.67\n3176\n1.128689\n\n\n2003-10-01\n11772.23\n3155\n1.380819\n\n\n2004-01-01\n11923.45\n3141\n1.701954\n\n\n2004-04-01\n12112.82\n3130\n1.358149\n\n\n2004-07-01\n12305.31\n3124\n1.933799"
  },
  {
    "objectID": "arimax.html#fit-the-model-using-auto.arima",
    "href": "arimax.html#fit-the-model-using-auto.arima",
    "title": "SARIMAX",
    "section": "Fit the model using ’auto.arima()`",
    "text": "Fit the model using ’auto.arima()`\n\n\nSeries: dd.ts[, \"Information Employment\"] \nRegression with ARIMA(0,0,2)(0,0,2)[4] errors \n\nCoefficients:\n         ma1     ma2    sma1    sma2  intercept     gdp       ir\n      1.0963  0.5950  0.3649  0.4137  2683.1158  0.0080  71.3895\ns.e.  0.1244  0.1404  0.1764  0.1813   116.2887  0.0067  18.2169\n\nsigma^2 = 2856:  log likelihood = -424.55\nAIC=865.09   AICc=867.15   BIC=884.05\n\nTraining set error measures:\n                    ME    RMSE      MAE        MPE     MAPE     MASE      ACF1\nTraining set -4.327941 51.0189 36.15202 -0.2118286 1.263825 0.524652 0.1997643\n\n\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from Regression with ARIMA(0,0,2)(0,0,2)[4] errors\nQ* = 25.63, df = 4, p-value = 3.757e-05\n\nModel df: 4.   Total lags used: 8"
  },
  {
    "objectID": "arimax.html#fit-the-model-manually",
    "href": "arimax.html#fit-the-model-manually",
    "title": "SARIMAX",
    "section": "Fit the model manually",
    "text": "Fit the model manually\n\nFirst fit the linear model:\n\n\n\nCall:\nlm(formula = `Information Employment` ~ GDP + `Interest Rate`, \n    data = dd)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-208.26  -72.25  -24.87   58.45  310.97 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(>|t|)    \n(Intercept)     2.574e+03  8.058e+01  31.949  < 2e-16 ***\nGDP             6.884e-03  3.937e-03   1.749   0.0844 .  \n`Interest Rate` 1.870e+02  2.035e+01   9.188 5.82e-14 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 108.1 on 76 degrees of freedom\nMultiple R-squared:  0.5658,    Adjusted R-squared:  0.5544 \nF-statistic: 49.52 on 2 and 76 DF,  p-value: 1.703e-14\n\n\n\n\nThen, look at the residuals\n\nWithout Difference\n\n\n\n\n\n\n\nOrdinary Difference\n\n\n\n\n\n\n\nOrdinary Difference & Seasonal Difference\n\n\n\n\n\n\n\n\nFind the model parameters.\n\n\n    p  d  q  P  D  Q      AIC      BIC     AICc\n1   0  0  0  0  1  0 907.6150 909.9325 907.6698\n2   0  1  0  0  1  0 854.4312 856.7353 854.4868\n3   0  0  0  0  1  1 908.1850 912.8200 908.3517\n4   0  1  0  0  1  1 828.7371 833.3452 828.9061\n5   0  0  0  1  1  0 908.9794 913.6143 909.1460\n6   0  1  0  1  1  0 845.9064 850.5145 846.0754\n7   0  0  0  1  1  1 908.4592 915.4117 908.7972\n8   0  1  0  1  1  1 829.5965 836.5087 829.9394\n9   0  0  0  2  1  0 905.3131 912.2656 905.6511\n10  0  1  0  2  1  0 841.9095 848.8217 842.2523\n11  0  0  0  2  1  1 906.2281 915.4980 906.7995\n12  0  0  1  0  1  0 859.2117 863.8467 859.3783\n13  0  1  1  0  1  0 856.3489 860.9571 856.5180\n14  0  0  1  0  1  1 857.7835 864.7360 858.1215\n15  0  1  1  0  1  1 830.6975 837.6097 831.0404\n16  0  0  1  1  1  0 859.1153 866.0678 859.4534\n17  0  1  1  1  1  0 847.5417 854.4539 847.8845\n18  0  0  1  1  1  1 858.8497 868.1196 859.4211\n19  0  0  1  2  1  0 858.5823 867.8523 859.1538\n20  0  0  2  0  1  0 859.5802 866.5326 859.9182\n21  0  1  2  0  1  0 851.5891 858.5013 851.9320\n22  0  0  2  0  1  1 852.2051 861.4751 852.7766\n23  0  0  2  1  1  0 856.8211 866.0911 857.3926\n24  1  0  0  0  1  0 857.4716 862.1066 857.6383\n25  1  1  0  0  1  0 856.3631 860.9712 856.5321\n26  1  0  0  0  1  1 837.9940 844.9465 838.3321\n27  1  1  0  0  1  1 830.7120 837.6242 831.0548\n28  1  0  0  1  1  0 851.0912 858.0436 851.4292\n29  1  1  0  1  1  0 847.6911 854.6033 848.0339\n30  1  0  0  1  1  1 838.2652 847.5351 838.8366\n31  1  0  0  2  1  0 848.0331 857.3031 848.6045\n32  1  0  1  0  1  0 856.6924 863.6448 857.0304\n33  1  1  1  0  1  0 851.3537 858.2659 851.6965\n34  1  0  1  0  1  1 839.6227 848.8927 840.1942\n35  1  0  1  1  1  0 849.4077 858.6776 849.9791\n36  1  0  2  0  1  0 857.4344 866.7044 858.0058\n37  2  0  0  0  1  0 856.3557 863.3081 856.6937\n38  2  1  0  0  1  0 857.8814 864.7936 858.2242\n39  2  0  0  0  1  1 839.7839 849.0538 840.3553\n40  2  0  0  1  1  0 850.2682 859.5382 850.8396\n41  2  0  1  0  1  0 854.5722 863.8422 855.1436\n42 NA NA NA NA NA NA       NA       NA       NA\n\n\n\n\n  p d q P D Q      AIC      BIC     AICc\n4 0 1 0 0 1 1 828.7371 833.3452 828.9061\n\n\n  p d q P D Q      AIC      BIC     AICc\n4 0 1 0 0 1 1 828.7371 833.3452 828.9061\n\n\n  p d q P D Q      AIC      BIC     AICc\n4 0 1 0 0 1 1 828.7371 833.3452 828.9061\n\n\nBest models: SARIMA(0,1,0)x(0,1,1)[4], auto.arima suggested: SARIMA(0,0,2)(0,0,2)[4]"
  },
  {
    "objectID": "arimax.html#model-comparison",
    "href": "arimax.html#model-comparison",
    "title": "SARIMAX",
    "section": "Model Comparison",
    "text": "Model Comparison\n\nModel Diagnositic\n\nSARIMA(0,1,0)x(0,1,1)[4] from Manual Fitting\n\n\n\n\n\n\n\nSARIMA(0,0,2)x(0,0,2)[4] from auto.arima()\n\n\n\n\n\nSARIMA(0,1,0)x(0,1,1)[4] is better\n\n\n\nCross Validation\n\n\n\n\n\n\n\n[1] 40.92118 78.38342 72.35350 92.02167\n\n\n[1] 49.26815 80.76242 96.15649 65.98401\n\n\nSARIMA(0,1,0)x(0,1,1)[4] is better based on low RMSE"
  },
  {
    "objectID": "arimax.html#best-model-fitting",
    "href": "arimax.html#best-model-fitting",
    "title": "SARIMAX",
    "section": "Best Model Fitting",
    "text": "Best Model Fitting\n\n\nSeries: dd$`Information Employment` \nRegression with ARIMA(0,1,0)(0,1,1)[4] errors \n\nCoefficients:\n         sma1     gdp       ir\n      -0.8780  0.0777  17.9824\ns.e.   0.0932  0.0101  11.1088\n\nsigma^2 = 895.3:  log likelihood = -357.88\nAIC=723.76   AICc=724.34   BIC=732.98\n\nTraining set error measures:\n                   ME    RMSE      MAE      MPE      MAPE      MASE      ACF1\nTraining set 6.463512 28.3659 18.21201 0.219529 0.6487785 0.2642997 0.1028286"
  },
  {
    "objectID": "arimax.html#forcast",
    "href": "arimax.html#forcast",
    "title": "SARIMAX",
    "section": "Forcast",
    "text": "Forcast\n\nUse auto.arima() to forcast the exogenous variables\n\nGDP\n\n\nSeries: dd$GDP \nARIMA(0,1,0) with drift \n\nCoefficients:\n         drift\n      190.0670\ns.e.   38.1874\n\nsigma^2 = 115225:  log likelihood = -564.7\nAIC=1133.41   AICc=1133.57   BIC=1138.12\n\nTraining set error measures:\n                    ME     RMSE      MAE         MPE      MAPE      MASE\nTraining set 0.1407936 335.1231 148.3509 -0.08681734 0.7823694 0.1808457\n                  ACF1\nTraining set -0.090376\n\n\n\n\nInterest Rate\n\n\nSeries: dd$`Interest Rate` \nARIMA(0,1,0) \n\nsigma^2 = 0.08934:  log likelihood = -16.48\nAIC=34.96   AICc=35.02   BIC=37.32\n\nTraining set error measures:\n                      ME     RMSE       MAE      MPE     MAPE      MASE\nTraining set 0.006190167 0.297008 0.2344719 15.76077 62.29658 0.5345643\n                     ACF1\nTraining set -0.008739477\n\n\n\n\n\nForecast with exogenous variables"
  },
  {
    "objectID": "arimax.html#results-and-findings",
    "href": "arimax.html#results-and-findings",
    "title": "SARIMAX",
    "section": "Results and Findings",
    "text": "Results and Findings\nWe could observe that SARIMA and SARIMAX models give us similar trajectory. When the exogenous variable is included, we have a tight confidence interval. That means the SARIMAX model is more precise."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "Hi guys! My name is Yujie Chen (NetID: yc986). I’m currently pursuing my Master’s degree in Data Science and Analytics at Georgetown University. I graduated with a bachelor’s degree with a major in Physical Oceanography. Outside of Data Science I love sports - swimming, running, climbing, and biking"
  }
]